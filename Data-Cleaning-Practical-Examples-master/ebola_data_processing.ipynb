{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2014 Ebola Outbreak Data\n",
    "\n",
    "[Caitlin Rivers' `ebola` GitHub repository](https://github.com/cmrivers/ebola) contains summarized reports of Ebola cases from three countries during the recent outbreak of the disease in West Africa. These data are licenced for both commercial and non-commercial use. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "![ebola](images/ebola.jpg)\n",
    "\n",
    "From these data files, we will use pandas to import them and create a single data frame that includes the **daily totals of new cases** for each country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/cmrivers/ebola/master/guinea_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(url_base+'2014-09-02.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we will need to develop row **masks** to extract the data we need across all files, without having to manually extract data from each file.\n",
    "\n",
    "Let's hack at one file to develop the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(url_base+'2014-09-02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent issues with capitalization, we will simply revert all labels to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_vars = sample.Description.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in extracting new cases only, we can use the **string accessor** attribute to look for key words that we would like to include or exclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_mask = (lower_vars.str.contains('new') \n",
    "             & (lower_vars.str.contains('case') | lower_vars.str.contains('suspect')) \n",
    "             & ~lower_vars.str.contains('non')\n",
    "             & ~lower_vars.str.contains('total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have instead used regular expressions to do the same thing.\n",
    "\n",
    "Finally, we are only interested in three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.loc[case_mask, ['Date', 'Description', 'Totals']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now embed this operation in a loop over all the filenames in the database. We first need to create a range of dates, since the data files are organized by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreak_dates = [str(date).split(' ')[0] for date in pd.date_range('2014-07-01', '2014-11-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for country in ('guinea', 'sl', 'liberia'):\n",
    "    \n",
    "    print('Getting {} data files'.format(country))\n",
    "    for date in outbreak_dates:\n",
    "        \n",
    "        file = 'https://raw.githubusercontent.com/cmrivers/ebola/master/{}_data/{}.csv'.format(country, date)\n",
    "        try:\n",
    "            data = pd.read_csv(file)\n",
    "            print('\\t{}'.format(date))\n",
    "        except Exception:\n",
    "            continue\n",
    "        \n",
    "        # Convert to lower case to avoid capitalization issues\n",
    "        data.columns = data.columns.str.lower()\n",
    "        # Column naming is inconsistent. These procedures deal with that.\n",
    "        keep_columns = ['date']\n",
    "        if 'description' in data.columns:\n",
    "            keep_columns.append('description')\n",
    "        else:\n",
    "            keep_columns.append('variable')\n",
    "            \n",
    "        if 'totals' in data.columns:\n",
    "            keep_columns.append('totals')\n",
    "        else:\n",
    "            keep_columns.append('national')\n",
    "            \n",
    "        # Index out the columns we need, and rename them\n",
    "        keep_data = data[keep_columns]\n",
    "        keep_data.columns = 'date', 'variable', 'totals'\n",
    "        \n",
    "        # Extract the rows we might want\n",
    "        lower_vars = keep_data.variable.str.lower()\n",
    "        # Of course we can also use regex to do this\n",
    "        case_mask = (lower_vars.str.contains('new') \n",
    "                     & (lower_vars.str.contains('case') | lower_vars.str.contains('suspect') \n",
    "                                                        | lower_vars.str.contains('confirm')) \n",
    "                     & ~lower_vars.str.contains('non')\n",
    "                     & ~lower_vars.str.contains('total'))\n",
    "        \n",
    "        keep_data = keep_data[case_mask].dropna()\n",
    "        \n",
    "        # Convert data types\n",
    "        keep_data['date'] = pd.to_datetime(keep_data.date)\n",
    "        keep_data['totals'] = keep_data.totals.astype(int)\n",
    "        \n",
    "        # Assign country label and append to datasets list\n",
    "        datasets.append(keep_data.assign(country=country))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list populated with `DataFrame` objects for each day and country, we can call `concat` to concatenate them into a single `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat(datasets)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works because the structure of each table was identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating indices\n",
    "\n",
    "Notice from above, however, that the index contains redundant integer index values. We can confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a new unique index by calling the `reset_index` method on the new data frame after we import it, which will generate a new ordered, unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat(datasets).reset_index(drop=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reindexing** allows users to manipulate the data labels in a DataFrame. It forces a DataFrame to conform to the new index, and optionally, fill in missing data if requested.\n",
    "\n",
    "A simple use of `reindex` is to alter the order of the rows. For example, records are currently ordered first by country then by day, since this is the order in which they were iterated over and imported. We might arbitrarily want to reverse the order, which is performed by passing the appropriate index values to `reindex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.reindex(all_data.index[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the reindexing operation is not performed \"in-place\"; the original `DataFrame` remains as it was, and the method returns a copy of the `DataFrame` with the new index. This is a common trait for pandas, and is a Good Thing.\n",
    "\n",
    "We may also wish to reorder the columns this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.reindex(columns=['date', 'country', 'variable', 'totals']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by operations\n",
    "\n",
    "One of pandas' most powerful features is the ability to perform operations on subgroups of a `DataFrame`. These so-called **group by** operations defines subunits of the dataset according to the values of one or more variabes in the `DataFrame`.\n",
    "\n",
    "For this data, we want to sum the new case counts by day and country; so we pass these two column names to the `groupby` method, then sum the `totals` column accross them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_grouped = all_data.groupby(['country', 'date'])\n",
    "daily_cases = all_data_grouped['totals'].sum()\n",
    "daily_cases.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting series retains a hierarchical index from the group by operation. Hence, we can index out the counts for a given country on a particular day by indexing with the appropriate tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cases[('liberia', '2014-09-02')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the data we have extracted is that there appear to be serious **outliers** in the Liberian counts. The values are much too large to be a daily count, even during a serious outbreak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cases.sort_values(ascending=False)\n",
    "daily_cases.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter these outliers using an appropriate threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cases = daily_cases[daily_cases<200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "pandas data structures have high-level methods for creating a variety of plots, which tends to be easier than generating the corresponding plot using matplotlib. \n",
    "\n",
    "For example, we may want to create a plot of the cumulative cases for each of the three countries. The easiest way to do this is to remove the hierarchical index, and create a `DataFrame` of three columns, which will result in three lines when plotted.\n",
    "\n",
    "First, call `unstack` to remove the hierarichical index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cases.unstack().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, transpose the resulting `DataFrame` to swap the rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cases.unstack().T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have missing values for some dates, we will assume that the counts for those days were zero (the actual counts for that day may have bee included in the next reporting day's data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cases.unstack().T.fillna(0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculate the cumulative sum for all the columns, and generate a line plot, which we get by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cases.unstack().T.fillna(0).cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "An alternative to filling days without case reports with zeros is to aggregate the data at a coarser time scale. New cases are often reported by week; we can use the `resample` method to summarize the data into weekly values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_cases = daily_cases.unstack().T.resample('W').sum()\n",
    "weekly_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_cases.cumsum().plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
